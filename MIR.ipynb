{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPdI99CTMoTDSvIk9sZjNFO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ameesha02/Blog_website/blob/master/MIR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lapQEa1NXGbA",
        "outputId": "593f38d9-f052-4f7d-b32c-8af2aa44aa50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top audio file indices for query: [66 58 12 94  6]\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import numpy as np\n",
        "import tensorflow_text as tf_text\n",
        "\n",
        "# 1. Audio feature extractor (pre-trained VGGish style model from TF Hub)\n",
        "audio_model_url = \"https://tfhub.dev/google/vggish/1\"\n",
        "audio_embedder = hub.KerasLayer(audio_model_url, input_shape=[None, 96, 64, 1], output_shape=[128], trainable=False)\n",
        "\n",
        "# 2. Text encoder (USE architecture as example, replace with RoBERTa TensorFlow model if available)\n",
        "text_model_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
        "text_embedder = hub.KerasLayer(text_model_url, trainable=False)\n",
        "\n",
        "# 3. Dual encoders model architecture for contrastive learning\n",
        "class AudioTextRetrievalModel(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.audio_embedder = audio_embedder\n",
        "        self.text_embedder = text_embedder\n",
        "\n",
        "    def call(self, audio_inputs, text_inputs):\n",
        "        # audio_inputs: Batch of mel spectrogram images (Batch, time, freq, channel)\n",
        "        # text_inputs: Batch of raw text strings\n",
        "\n",
        "        audio_emb = self.audio_embedder(audio_inputs)          # (batch, 128)\n",
        "        text_emb = self.text_embedder(text_inputs)             # (batch, 512)\n",
        "\n",
        "        # Normalize embeddings for cosine similarity\n",
        "        audio_emb = tf.math.l2_normalize(audio_emb, axis=1)\n",
        "        text_emb = tf.math.l2_normalize(text_emb, axis=1)\n",
        "        return audio_emb, text_emb\n",
        "\n",
        "# 4. Contrastive loss using cosine similarity and temperature scaling\n",
        "def contrastive_loss(audio_emb, text_emb, temperature=0.07):\n",
        "    logits = tf.matmul(audio_emb, text_emb, transpose_b=True) / temperature\n",
        "    batch_size = tf.shape(audio_emb)[0]\n",
        "    labels = tf.range(batch_size)\n",
        "    loss_a2t = tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "    loss_t2a = tf.keras.losses.sparse_categorical_crossentropy(labels, tf.transpose(logits), from_logits=True)\n",
        "    return tf.reduce_mean(loss_a2t + loss_t2a)\n",
        "\n",
        "# 5. Example dummy pipeline for training with batched audio and text data (pseudocode)\n",
        "@tf.function\n",
        "def train_step(model, audio_batch, text_batch, optimizer):\n",
        "    with tf.GradientTape() as tape:\n",
        "        audio_emb, text_emb = model(audio_batch, text_batch)\n",
        "        loss = contrastive_loss(audio_emb, text_emb)\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    return loss\n",
        "\n",
        "# 6. Build and train model (trainer loop simplified)\n",
        "model = AudioTextRetrievalModel()\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
        "\n",
        "# Assume audio_dataset and text_dataset are tf.data.Dataset batches of spectrograms and texts aligned (not shown here)\n",
        "# for epoch in range(num_epochs):\n",
        "#     for audio_batch, text_batch in zip(audio_dataset, text_dataset):\n",
        "#         loss = train_step(model, audio_batch, text_batch, optimizer)\n",
        "#         print(f'Training loss: {loss.numpy()}')\n",
        "\n",
        "# 7. Inference example for audio retrieval given query text:\n",
        "text_model_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
        "text_embedder = hub.KerasLayer(text_model_url, trainable=False)\n",
        "\n",
        "# Dummy audio embeddings database as normalized vectors (e.g. 100 audio clips with 512-dim embeddings)\n",
        "num_audio = 100\n",
        "embedding_dim = 512\n",
        "np.random.seed(42)\n",
        "audio_embeddings_db = np.random.rand(num_audio, embedding_dim).astype(np.float32)\n",
        "audio_embeddings_db /= np.linalg.norm(audio_embeddings_db, axis=1, keepdims=True)  # L2 normalize\n",
        "audio_embeddings_db = tf.constant(audio_embeddings_db)\n",
        "\n",
        "def retrieve_audio(audio_embeddings_db, query_text, text_embedder, top_k=5):\n",
        "    query_emb = text_embedder(tf.constant([query_text]))\n",
        "    query_emb = tf.math.l2_normalize(query_emb, axis=1)\n",
        "    similarities = tf.linalg.matmul(audio_embeddings_db, query_emb, transpose_b=True)\n",
        "    top_k_indices = tf.math.top_k(tf.squeeze(similarities), k=top_k).indices.numpy()\n",
        "    return top_k_indices\n",
        "\n",
        "# Example query text\n",
        "query_text = \"rain and thunder sounds\"\n",
        "\n",
        "# Run retrieval\n",
        "top_results = retrieve_audio(audio_embeddings_db, query_text, text_embedder, top_k=5)\n",
        "\n",
        "print(\"Top audio file indices for query:\", top_results)\n",
        "\n",
        "# Note: This code illustrates main blocks; real implementation requires:\n",
        "# - Proper audio preprocessing to mel spectrograms matching input shape\n",
        "# - Efficient dataset loading and batching\n",
        "# - Building an indexed audio embedding database for retrieval\n",
        "# - Using domain-specific pretrained models (RoBERTa text, ResNet/PANN audio encoder)\n",
        "# - Training with augmented data to capture temporal/contextual audio-text relationships\n",
        "\n"
      ]
    }
  ]
}